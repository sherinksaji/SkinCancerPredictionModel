{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69872a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a7b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path to the directory containing all images\n",
    "image_dir = 'HAM10000_images'\n",
    "\n",
    "# Path to the CSV file containing image labels\n",
    "metadata_path = 'HAM10000_metadata.csv'\n",
    "\n",
    "# Directories for the split datasets\n",
    "train_dir = 'train'\n",
    "test_dir = 'test'\n",
    "validation_dir = 'valid'\n",
    "\n",
    "# Creating directories if they don't exist\n",
    "for directory in [train_dir, test_dir, validation_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca81e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         lesion_id      image_id     dx dx_type   age     sex localization  \\\n",
      "0      HAM_0000118  ISIC_0027419    bkl   histo  80.0    male        scalp   \n",
      "1      HAM_0000118  ISIC_0025030    bkl   histo  80.0    male        scalp   \n",
      "2      HAM_0002730  ISIC_0026769    bkl   histo  80.0    male        scalp   \n",
      "3      HAM_0002730  ISIC_0025661    bkl   histo  80.0    male        scalp   \n",
      "4      HAM_0001466  ISIC_0031633    bkl   histo  75.0    male          ear   \n",
      "...            ...           ...    ...     ...   ...     ...          ...   \n",
      "10010  HAM_0002867  ISIC_0033084  akiec   histo  40.0    male      abdomen   \n",
      "10011  HAM_0002867  ISIC_0033550  akiec   histo  40.0    male      abdomen   \n",
      "10012  HAM_0002867  ISIC_0033536  akiec   histo  40.0    male      abdomen   \n",
      "10013  HAM_0000239  ISIC_0032854  akiec   histo  80.0    male         face   \n",
      "10014  HAM_0003521  ISIC_0032258    mel   histo  70.0  female         back   \n",
      "\n",
      "       target  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "...       ...  \n",
      "10010       0  \n",
      "10011       0  \n",
      "10012       0  \n",
      "10013       0  \n",
      "10014       1  \n",
      "\n",
      "[10015 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "target=list(map(lambda x: 1 if x=='mel' else 0,metadata['dx']))\n",
    "metadata['target']=target\n",
    "print(metadata)\n",
    "# Splitting the dataset into train, validation, and test sets\n",
    "#train_val, test = train_test_split(metadata, test_size=0.2, random_state=42)\n",
    "#train, validation = train_test_split(train_val, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa9771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images(df, source_dir, target_dir):\n",
    "    for _, row in df.iterrows():\n",
    "        filename = row['image_id'] + '.jpg'  # Assuming image IDs in the CSV and filenames match\n",
    "        source_path = os.path.join(source_dir, filename)\n",
    "        target_path = os.path.join(target_dir, filename)\n",
    "        \n",
    "        # Move the image\n",
    "        shutil.move(source_path, target_path)\n",
    "\n",
    "# Moving the images\n",
    "move_images(train, image_dir, train_dir)\n",
    "move_images(validation, image_dir, validation_dir)\n",
    "move_images(test, image_dir, test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec540cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_images = [f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))]\n",
    "test_images = [f for f in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, f))]\n",
    "validation_images = [f for f in os.listdir(validation_dir) if os.path.isfile(os.path.join(validation_dir, f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_target_to_images(images):    \n",
    "    metadata_dict = pd.Series(metadata['dx'].values,index=metadata['image_id']).to_dict()\n",
    "    images_with_target = []\n",
    "    for image in images:\n",
    "        image_id = image.split('.')[0]  # Assuming image_id does not contain '.'\n",
    "        target = 1 if metadata_dict.get(image_id) == 'mel' else 0\n",
    "        train_image_with_label=(image,target)\n",
    "        images_with_target.append(train_image_with_label)\n",
    "    return images_with_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eac4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_with_targets=append_target_to_images(train_images)\n",
    "test_images_with_targets=append_target_to_images(test_images)\n",
    "validation_images_with_targets=append_target_to_images(validation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccf232",
   "metadata": {},
   "outputs": [],
   "source": [
    "melanoma_count_in_train=sum(map(lambda x:x[1],train_images_with_targets))\n",
    "melanoma_count_in_test=sum(map(lambda x:x[1],test_images_with_targets))\n",
    "melanoma_count_in_validation=sum(map(lambda x:x[1],validation_images_with_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Count of people with melanoma in train_images_with_targets:{melanoma_count_in_train}\")\n",
    "print(f\"Count of people without melanoma in train_images_with_targets:{len(train_images_with_targets)-melanoma_count_in_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Count of people with melanoma in test_images_with_targets:{melanoma_count_in_test}\")\n",
    "print(f\"Count of people without melanoma in test_images_with_targets:{len(test_images_with_targets)-melanoma_count_in_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Count of people with melanoma in validation_images_with_targets:{melanoma_count_in_validation}\")\n",
    "print(f\"Count of people without melanoma in validation_images_with_targets:{len(validation_images_with_targets)-melanoma_count_in_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8776f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def move_on_class(imagelist,dirs):\n",
    "    for image_name, label in imagelist:\n",
    "        mel_dir = os.path.join(dirs, 'mel')\n",
    "        no_mel_dir = os.path.join(dirs, 'no_mel')\n",
    "        os.makedirs(mel_dir, exist_ok=True)\n",
    "        os.makedirs(no_mel_dir, exist_ok=True)\n",
    "        source_dir=dirs\n",
    "        # Determine the source path of the image\n",
    "        source_path = os.path.join(source_dir, image_name)\n",
    "\n",
    "        # Determine the destination path based on the label\n",
    "        if label == 1:  # Melanoma\n",
    "            dest_path = os.path.join(mel_dir, image_name)\n",
    "        else:  # Non-melanoma\n",
    "            dest_path = os.path.join(no_mel_dir, image_name)\n",
    "            \n",
    "\n",
    "        # Move the image from source to destination\n",
    "        shutil.move(source_path, dest_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac401b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move_on_class(train_images_with_targets,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move_on_class(test_images_with_targets,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e4d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move_on_class(validation_images_with_targets,'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eacfd1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Custom Gaussian Noise Transform\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "# Correctly ordered transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness and contrast\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
    "    transforms.RandomRotation(20),  # Randomly rotate images by 20 degrees\n",
    "    transforms.ToTensor(),  # Convert PIL Image to Tensor\n",
    "    AddGaussianNoise(0., 0.1),  # Add custom Gaussian Noise\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16cdea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000229CBB78970>\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "# Create datasets using ImageFolder\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "valid_dataset = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size =12  # Set this to something appropriate for your hardware\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f29bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 6009\n",
      "Number of batches in train_loader: 501\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06e388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f178bfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Load the pre-trained DenseNet201 and MobileNetV2 models\n",
    "densenet = models.densenet201(pretrained=True)\n",
    "mobilenet = models.mobilenet_v2(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d976aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# DenseNet201: Replace classifier with an identity layer to keep features\n",
    "densenet.classifier = torch.nn.Identity()\n",
    "\n",
    "# MobileNetV2: Replace classifier with an identity layer as well\n",
    "mobilenet.classifier = torch.nn.Identity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d850db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=512, bias=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "class CustomClassificationHead(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(CustomClassificationHead, self).__init__()\n",
    "        # No changes here as this part is not as memory intensive\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, 512)  # Consider reducing size if necessary\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(512, num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the features\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.log_softmax(x)\n",
    "        return x\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        densenet = models.densenet201(pretrained=True)\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        \n",
    "        # Remove classification layers\n",
    "        densenet.classifier = nn.Identity()\n",
    "        mobilenet.classifier = nn.Sequential(*list(mobilenet.classifier.children())[:-1], nn.Identity())\n",
    "        \n",
    "        self.densenet = densenet\n",
    "        self.mobilenet = mobilenet\n",
    "        \n",
    "        # Feature reduction layer\n",
    "        self.feature_reduction = nn.Linear(1920 + 1280, 1024)  # Example: Reducing to 1024 features\n",
    "        \n",
    "        # Adjusted classification head for reduced feature size\n",
    "        self.classification_head = CustomClassificationHead(num_features=1024, num_classes=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Using checkpointing for memory efficiency\n",
    "        features_densenet = checkpoint(self.densenet, x)\n",
    "        features_mobilenet = checkpoint(self.mobilenet, x)\n",
    "        \n",
    "        # Concatenate features along the feature dimension\n",
    "        features_combined = torch.cat((features_densenet, features_mobilenet), dim=1)\n",
    "        \n",
    "        # Reduce feature size\n",
    "        reduced_features = self.feature_reduction(features_combined)\n",
    "        \n",
    "        # Forward pass through the classification head\n",
    "        x = self.classification_head(reduced_features)\n",
    "        return x\n",
    "\n",
    "# Initialize the combined model\n",
    "combined_model = CombinedModel()\n",
    "\n",
    "# Assuming 'device' is defined (e.g., cuda or cpu)\n",
    "combined_model=combined_model.to(device)\n",
    "\n",
    "\n",
    "# Example: Pruning 20% of connections in the dense1 layer of the classification head by weight magnitude\n",
    "prune.l1_unstructured(combined_model.classification_head.dense1, name='weight', amount=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d4e8735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear unused memory\n",
    "\n",
    "inputs, labels = next(iter(train_loader))\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "outputs = combined_model(inputs)  # Check if this line hangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71068de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(combined_model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd87489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|█████████████████████████████████████████████████████████| 501/501 [02:28<00:00,  3.38it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████| 167/167 [00:46<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.2847, Train Acc: 89.03%, Valid Loss: 0.2861, Valid Acc: 89.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|█████████████████████████████████████████████████████████| 501/501 [02:24<00:00,  3.46it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████| 167/167 [00:46<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.2731, Train Acc: 89.28%, Valid Loss: 0.3110, Valid Acc: 88.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|█████████████████████████████████████████████████████████| 501/501 [02:25<00:00,  3.44it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████| 167/167 [00:46<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.2740, Train Acc: 89.42%, Valid Loss: 0.2738, Valid Acc: 89.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|█████████████████████████████████████████████████████████| 501/501 [02:26<00:00,  3.43it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████| 167/167 [00:46<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.2692, Train Acc: 89.53%, Valid Loss: 0.2635, Valid Acc: 89.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|█████████████████████████████████████████████████████████| 501/501 [02:25<00:00,  3.44it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████| 167/167 [00:46<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.2640, Train Acc: 89.63%, Valid Loss: 0.2644, Valid Acc: 89.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|█████████████████████████████████████████████████████████| 501/501 [02:26<00:00,  3.43it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████| 167/167 [00:46<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.2701, Train Acc: 89.53%, Valid Loss: 0.2528, Valid Acc: 89.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|█████████████████████████████████████████████████████████| 501/501 [02:26<00:00,  3.43it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████| 167/167 [00:46<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.2619, Train Acc: 89.50%, Valid Loss: 0.2496, Valid Acc: 89.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|█████████████████████████████████████████████████████████| 501/501 [02:26<00:00,  3.42it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████| 167/167 [00:46<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.2533, Train Acc: 89.95%, Valid Loss: 0.2516, Valid Acc: 89.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|█████████████████████████████████████████████████████████| 501/501 [02:26<00:00,  3.41it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████| 167/167 [00:46<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.2575, Train Acc: 89.77%, Valid Loss: 0.2710, Valid Acc: 89.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|████████████████████████████████████████████████████████| 501/501 [02:25<00:00,  3.43it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████| 167/167 [00:46<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.2557, Train Acc: 89.95%, Valid Loss: 0.2573, Valid Acc: 89.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    combined_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = combined_model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = correct / total\n",
    "    \n",
    "    # Validation phase\n",
    "    combined_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(valid_loader, desc=\"Validating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = combined_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    valid_accuracy = correct / total\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "          f\"Train Acc: {train_accuracy * 100:.2f}%, \"\n",
    "          f\"Valid Loss: {valid_loss / len(valid_loader):.4f}, \"\n",
    "          f\"Valid Acc: {valid_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf86ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
